# Module 3 Training – Code Quality & Review

## Overview

AI coding assistants like GitHub Copilot and Roo Code are now embedded in daily software development.  According to a 2024 Stack Overflow survey, **76 percent of developers use AI to help write software**, and Microsoft reports that **about 30 percent of its code is written by AI**【846912144979599†L155-L163】.  As AI‑generated code becomes ubiquitous, it also introduces risks: missing input validation, memory management problems, insecure dependencies and secrets, and even hallucinated package names【846912144979599†L188-L201】【846912144979599†L247-L256】.  Module 3 focuses on ensuring quality, correctness and security of AI‑produced code through a disciplined review process.  You will learn how to identify common pitfalls, apply static and dynamic analysis, conduct thorough manual reviews, and combine AI‑assisted tools with human judgement.

This document contains:

1. **Theory and key concepts** – covering the ubiquity of AI‑generated code, typical vulnerabilities, differences between static and dynamic analysis, manual vs AI‑assisted review, and best practices.
2. **Labs and tasks** – three hands‑on labs that guide you through static analysis, manual peer review and security scanning.
3. **Assessment plan** – a short quiz and a reflection template to consolidate your learning.

---

## 1. Theory and Key Concepts

### 1.1 Ubiquity of AI‑generated code

AI assistance is no longer a novelty – it’s the norm.  Developers leverage tools like Copilot to boost productivity, with **76 % of coders using AI** and enterprises such as Microsoft seeing **around 30 % of code being generated by AI**【846912144979599†L155-L163】.  This adoption accelerates development but also creates a larger surface area for bugs and vulnerabilities.  Because AI code may bypass traditional peer review, it must be scrutinised to the same (or higher) standard as human‑written code.

### 1.2 Common pitfalls and vulnerabilities

AI models can produce syntactically correct but logically flawed code.  Common issues include:

| Issue | Description | Example / Consequence |
| --- | --- | --- |
| **Lack of input validation** | Models may not perform proper bounds checking or sanitisation, leading to injection attacks, buffer overflows and other vulnerabilities【846912144979599†L188-L201】. | A generated API might accept untrusted input and directly interpolate it into a SQL query. |
| **Memory management problems** | AI may omit resource cleanup or mishandle memory, causing leaks and crashes【846912144979599†L188-L201】. | In a C++ program, failing to free allocated memory could exhaust resources. |
| **Insecure dependencies** | Generated code often pulls in external libraries without verifying their safety.  It might reference non‑existent or malicious packages – a phenomenon called **package hallucination**【846912144979599†L247-L256】. | An attacker could upload a malicious package with the hallucinated name, leading to compromise when the app installs it. |
| **Hard‑coded secrets** | Models sometimes insert access tokens or passwords directly into code【846912144979599†L188-L201】. | Secrets committed to source control can be harvested by attackers. |

You should treat AI‑generated code like any third‑party dependency: review it thoroughly, scan it for vulnerabilities, and verify package names before installation.

### 1.3 Static vs dynamic analysis

**Static analysis** inspects source code without executing it.  It scans for structural issues, coding standard violations and security flaws【578077277023639†L141-L149】.  Tools such as linters, type checkers and SAST (Static Application Security Testing) can detect unused variables, insecure patterns and potential injection points early in the development process.  In contrast, **dynamic analysis** exercises the code at runtime to uncover issues that only surface during execution, such as memory leaks, race conditions and performance bottlenecks【578077277023639†L155-L159】.  Dynamic tests include unit tests, integration tests and DAST (Dynamic Application Security Testing).  Both approaches are complementary: static analysis catches many problems quickly, while dynamic analysis verifies actual behaviour【578077277023639†L141-L160】.

### 1.4 Manual vs AI‑assisted review

Human insight remains irreplaceable.  Manual code review lets you assess design intent, readability and domain‑specific constraints that an AI may not understand.  When reviewing AI‑generated code, pay attention to:

* **Correctness** – Does the code actually implement the requirements?  Are edge cases handled?  Are tests thorough?
* **Security** – Are inputs validated?  Are secrets handled securely?  Are external calls safe?  Use SCA/SAST/DAST tools to catch hidden issues【846912144979599†L281-L296】.
* **Maintainability** – Is the code readable and consistent with your project’s style?  Are functions appropriately sized and named?

AI‑assisted reviewers (e.g. Roo Code’s “Explain Code” or GitHub’s AI code review features) can highlight suspicious patterns and suggest fixes.  However, they should complement, not replace, human review.  Always verify AI suggestions and consult colleagues when in doubt.

### 1.5 Best practices

1. **Combine analyses:** Use static analysis, dynamic testing and multiple security scans (SCA, SAST, DAST) to cover different risk surfaces【846912144979599†L281-L296】.  No single tool is sufficient【846912144979599†L299-L301】.
2. **Verify dependencies:** Check that all third‑party packages exist and are trusted before installation; watch for hallucinated names【846912144979599†L247-L256】.
3. **Enforce input validation and error handling:** Always sanitise inputs and handle exceptions properly.  Consider using established frameworks or libraries for validation.
4. **Keep secrets out of source control:** Use environment variables or secrets management services rather than embedding credentials in code.
5. **Write clear commit messages and small diffs:** When integrating AI suggestions, commit incrementally and explain the rationale.  This makes reviews manageable and enables easier rollback.
6. **Review early and often:** Don’t wait until the end of a sprint.  Perform lightweight reviews on each pull request to catch issues before they propagate.

---

## 2. Labs and Tasks

There are three labs in this module.  Each lab is provided in the `labs` folder and contains step‑by‑step instructions with reflection questions.  The labs build upon your experience from previous modules and emphasise review skills.

| Lab | Focus | Key skills |
| --- | --- | --- |
| [Lab 1](labs/lab1.md) | Static analysis & code fixing | Running linters (`flake8`, `pylint`), type checkers (`mypy`), and Bandit to detect security issues; refining AI prompts to produce better code; manually correcting issues and running unit tests. |
| [Lab 2](labs/lab2.md) | Manual & peer code review | Creating pull requests, reviewing diffs, spotting logical errors and style issues, writing constructive comments, iterating on AI‑generated code with human judgement. |
| [Lab 3](labs/lab3.md) | Security & dependency scanning | Using Bandit and pip‑audit (or other SCA tools) to identify insecure dependencies and secrets; verifying package names to avoid hallucinated dependencies; remediating vulnerabilities. |

Each lab ends with reflection questions to encourage critical thinking about the AI’s output and your role in supervising it.

---

## 3. Assessment Plan

### 3.1 Quiz

The quiz in **`assessments/quiz.md`** comprises multiple‑choice and short‑answer questions.  Topics include the adoption of AI‑generated code, common vulnerabilities, differences between static and dynamic analysis, security testing tools, and best practices for code review.

### 3.2 Reflection template

Learners should complete **`assessment_reflection.md`** after finishing the labs.  Use it to summarise the tasks you performed, enumerate vulnerabilities and issues you discovered, describe how you fixed them, and propose improvements to your prompts and review workflow.

### 3.3 Evaluation

Trainers or team leads can evaluate quiz answers and reflections.  Success criteria may include:

* Score ≥ 80 % on the quiz.
* Demonstrated understanding of common AI‑generated code vulnerabilities and how to detect them.
* Evidence of applying static and dynamic analysis tools appropriately.
* Thoughtful reflections showing critical judgement of AI suggestions and meaningful improvements.

---

## 4. Files Included

This training pack includes the following files:

* **module3_training_pack.md** – The document you’re reading, containing theory, labs and assessments overview.
* **labs/lab1.md** – Instructions for running static analysis and fixing AI‑generated code.
* **labs/lab2.md** – Instructions for performing manual and peer code reviews on AI‑generated code.
* **labs/lab3.md** – Tasks for security scanning and dependency auditing.
* **assessments/quiz.md** – Quiz for evaluating knowledge of this module.
* **assessment_reflection.md** – Template for reflecting on your code review experience.
* **slides/module3.pptx** – The summary slide deck for Module 3 (see `Slides/Module_3/module3.pptx`).

Use this package as the foundation for your training sessions on AI‑augmented code review.